#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºè”æ‹›è˜çˆ¬è™«è¿è¡Œè„šæœ¬
ä¸“é—¨çˆ¬å–è‹å·å·¥ä¸šå›­åŒºäººå·¥æ™ºèƒ½ç›¸å…³å²—ä½
"""

import sys
import os
from datetime import datetime

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from zhilian_ai_scraper import ZhilianAIScraper
from init_zhilian_db import init_zhilian_db


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸš€ æ™ºè”æ‹›è˜çˆ¬è™« - è‹å·å·¥ä¸šå›­åŒºäººå·¥æ™ºèƒ½å²—ä½")
    print("=" * 60)
    
    # åˆå§‹åŒ–æ•°æ®åº“
    print("\nğŸ“‹ æ­¥éª¤1: åˆå§‹åŒ–æ•°æ®åº“...")
    try:
        init_zhilian_db()
        print("âœ… æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    print("\nğŸ•·ï¸  æ­¥éª¤2: åˆ›å»ºçˆ¬è™«å®ä¾‹...")
    scraper = ZhilianAIScraper()
    
    # é…ç½®çˆ¬å–å‚æ•°
    config = {
        'keyword': 'äººå·¥æ™ºèƒ½',
        'city': 'è‹å·',
        'max_pages': 3  # å…ˆçˆ¬å–3é¡µæµ‹è¯•
    }
    
    print(f"ğŸ“ çˆ¬å–é…ç½®:")
    print(f"   å…³é”®è¯: {config['keyword']}")
    print(f"   åŸå¸‚: {config['city']}")
    print(f"   æœ€å¤§é¡µæ•°: {config['max_pages']}")
    
    # å¼€å§‹çˆ¬å–
    print(f"\nğŸ¯ æ­¥éª¤3: å¼€å§‹çˆ¬å–èŒä½ä¿¡æ¯...")
    print("âš ï¸  æ³¨æ„: è¯·ç¡®ä¿Chromeæµè§ˆå™¨å·²å®‰è£…")
    print("âš ï¸  çˆ¬å–è¿‡ç¨‹å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...")
    
    try:
        jobs = scraper.scrape_jobs(
            keyword=config['keyword'],
            city=config['city'],
            max_pages=config['max_pages']
        )
        
        if jobs:
            print(f"\nğŸ‰ çˆ¬å–æˆåŠŸï¼å…±è·å–åˆ° {len(jobs)} ä¸ªèŒä½")
            
            # æ˜¾ç¤ºå‰å‡ ä¸ªèŒä½çš„ä¿¡æ¯
            print("\nğŸ“Š èŒä½é¢„è§ˆ:")
            for i, job in enumerate(jobs[:5]):  # æ˜¾ç¤ºå‰5ä¸ª
                print(f"  {i+1}. {job.get('job_title', 'N/A')} - {job.get('company_name', 'N/A')}")
                print(f"     è–ªèµ„: {job.get('salary', 'N/A')} | åœ°ç‚¹: {job.get('work_city', 'N/A')}")
                print(f"     ç»éªŒ: {job.get('work_experience', 'N/A')} | å­¦å†: {job.get('education', 'N/A')}")
                print()
            
            if len(jobs) > 5:
                print(f"   ... è¿˜æœ‰ {len(jobs) - 5} ä¸ªèŒä½")
            
            # å¯¼å‡ºCSV
            print("\nğŸ“„ æ­¥éª¤4: å¯¼å‡ºCSVæ–‡ä»¶...")
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            csv_filename = f"è‹å·å·¥ä¸šå›­åŒºäººå·¥æ™ºèƒ½å²—ä½_{timestamp}.csv"
            
            scraper.export_to_csv(jobs, csv_filename)
            print(f"âœ… CSVæ–‡ä»¶å·²ä¿å­˜: {csv_filename}")
            
            # ç»Ÿè®¡ä¿¡æ¯
            print("\nğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
            companies = set(job.get('company_name', '') for job in jobs if job.get('company_name'))
            print(f"   æ¶‰åŠå…¬å¸æ•°é‡: {len(companies)}")
            
            salaries = [job.get('salary', '') for job in jobs if job.get('salary')]
            print(f"   æœ‰è–ªèµ„ä¿¡æ¯çš„èŒä½: {len(salaries)}")
            
            experiences = [job.get('work_experience', '') for job in jobs if job.get('work_experience')]
            print(f"   æœ‰ç»éªŒè¦æ±‚çš„èŒä½: {len(experiences)}")
            
        else:
            print("âŒ æ²¡æœ‰çˆ¬å–åˆ°ä»»ä½•èŒä½ä¿¡æ¯")
            print("å¯èƒ½åŸå› :")
            print("  1. ç½‘ç»œè¿æ¥é—®é¢˜")
            print("  2. æ™ºè”æ‹›è˜ç½‘ç«™ç»“æ„å˜åŒ–")
            print("  3. è¢«åçˆ¬è™«æœºåˆ¶æ‹¦æˆª")
            print("  4. æœç´¢æ¡ä»¶è¿‡äºä¸¥æ ¼")
            
    except Exception as e:
        print(f"âŒ çˆ¬å–è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("å»ºè®®:")
        print("  1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("  2. ç¡®è®¤Chromeæµè§ˆå™¨å·²å®‰è£…")
        print("  3. å°è¯•å‡å°‘çˆ¬å–é¡µæ•°")
        print("  4. æ£€æŸ¥æ•°æ®åº“è¿æ¥")
    
    print("\n" + "=" * 60)
    print("ğŸ çˆ¬è™«è¿è¡Œå®Œæˆ")
    print("=" * 60)


if __name__ == "__main__":
    main() 