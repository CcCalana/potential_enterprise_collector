#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºè”æ‹›è˜çˆ¬è™«æµ‹è¯•è„šæœ¬
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from zhilian_ai_scraper import ZhilianAIScraper
from init_zhilian_db import init_zhilian_db


def test_database_init():
    """æµ‹è¯•æ•°æ®åº“åˆå§‹åŒ–"""
    print("=== æµ‹è¯•æ•°æ®åº“åˆå§‹åŒ– ===")
    try:
        engine = init_zhilian_db()
        print("âœ… æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âŒ æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
        return False


def test_url_building():
    """æµ‹è¯•URLæ„å»º"""
    print("\n=== æµ‹è¯•URLæ„å»º ===")
    scraper = ZhilianAIScraper()
    
    # æµ‹è¯•é»˜è®¤å‚æ•°
    url1 = scraper.build_search_url()
    print(f"é»˜è®¤URL: {url1}")
    
    # æµ‹è¯•è‡ªå®šä¹‰å‚æ•°
    url2 = scraper.build_search_url(keyword="æœºå™¨å­¦ä¹ ", city="è‹å·", page=2)
    print(f"è‡ªå®šä¹‰URL: {url2}")
    
    # éªŒè¯URLæ ¼å¼
    expected_pattern = "https://www.zhaopin.com/sou/jl538/kw"
    if expected_pattern in url1 and expected_pattern in url2:
        print("âœ… URLæ„å»ºæµ‹è¯•é€šè¿‡")
        return True
    else:
        print("âŒ URLæ„å»ºæµ‹è¯•å¤±è´¥")
        return False


def test_time_parsing():
    """æµ‹è¯•æ—¶é—´è§£æ"""
    print("\n=== æµ‹è¯•æ—¶é—´è§£æ ===")
    scraper = ZhilianAIScraper()
    
    test_cases = [
        "ä»Šå¤©",
        "æ˜¨å¤©", 
        "3å¤©å‰",
        "12-25",
        "01-15"
    ]
    
    all_passed = True
    for time_str in test_cases:
        result = scraper.parse_publish_time(time_str)
        if result:
            print(f"âœ… '{time_str}' -> {result}")
        else:
            print(f"âŒ '{time_str}' -> None")
            all_passed = False
    
    return all_passed


def test_basic_scraping():
    """æµ‹è¯•åŸºæœ¬çˆ¬å–åŠŸèƒ½ï¼ˆåªçˆ¬å–1é¡µï¼‰"""
    print("\n=== æµ‹è¯•åŸºæœ¬çˆ¬å–åŠŸèƒ½ ===")
    
    scraper = ZhilianAIScraper()
    
    try:
        # åªçˆ¬å–1é¡µè¿›è¡Œæµ‹è¯•
        jobs = scraper.scrape_jobs(
            keyword="äººå·¥æ™ºèƒ½",
            city="è‹å·",
            max_pages=1
        )
        
        if jobs:
            print(f"âœ… æˆåŠŸçˆ¬å–åˆ° {len(jobs)} ä¸ªèŒä½")
            
            # æ˜¾ç¤ºç¬¬ä¸€ä¸ªèŒä½çš„ä¿¡æ¯
            if jobs:
                first_job = jobs[0]
                print(f"ç¤ºä¾‹èŒä½: {first_job.get('job_title', 'N/A')}")
                print(f"å…¬å¸: {first_job.get('company_name', 'N/A')}")
                print(f"è–ªèµ„: {first_job.get('salary', 'N/A')}")
                print(f"åœ°ç‚¹: {first_job.get('work_city', 'N/A')}")
            
            return True
        else:
            print("âŒ æ²¡æœ‰çˆ¬å–åˆ°ä»»ä½•èŒä½æ•°æ®")
            return False
            
    except Exception as e:
        print(f"âŒ çˆ¬å–æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_csv_export():
    """æµ‹è¯•CSVå¯¼å‡ºåŠŸèƒ½"""
    print("\n=== æµ‹è¯•CSVå¯¼å‡ºåŠŸèƒ½ ===")
    
    scraper = ZhilianAIScraper()
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_jobs = [
        {
            'job_title': 'äººå·¥æ™ºèƒ½å·¥ç¨‹å¸ˆ',
            'company_name': 'æµ‹è¯•å…¬å¸A',
            'salary': '15K-25K',
            'work_city': 'è‹å·',
            'work_experience': '3-5å¹´',
            'education': 'æœ¬ç§‘',
            'company_size': '100-499äºº',
            'welfare': 'äº”é™©ä¸€é‡‘',
            'publish_time': '2024-01-15',
            'job_url': 'https://www.zhaopin.com/job/test1.html'
        },
        {
            'job_title': 'æœºå™¨å­¦ä¹ å·¥ç¨‹å¸ˆ',
            'company_name': 'æµ‹è¯•å…¬å¸B',
            'salary': '20K-30K',
            'work_city': 'è‹å·',
            'work_experience': '5-10å¹´',
            'education': 'ç¡•å£«',
            'company_size': '500-999äºº',
            'welfare': 'äº”é™©ä¸€é‡‘,å¹´ç»ˆå¥–',
            'publish_time': '2024-01-14',
            'job_url': 'https://www.zhaopin.com/job/test2.html'
        }
    ]
    
    try:
        scraper.export_to_csv(test_jobs, "test_jobs.csv")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if os.path.exists("test_jobs.csv"):
            print("âœ… CSVå¯¼å‡ºæµ‹è¯•é€šè¿‡")
            
            # æ¸…ç†æµ‹è¯•æ–‡ä»¶
            os.remove("test_jobs.csv")
            return True
        else:
            print("âŒ CSVæ–‡ä»¶æœªç”Ÿæˆ")
            return False
            
    except Exception as e:
        print(f"âŒ CSVå¯¼å‡ºæµ‹è¯•å¤±è´¥: {e}")
        return False


def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹è¿è¡Œæ™ºè”æ‹›è˜çˆ¬è™«æµ‹è¯•å¥—ä»¶...\n")
    
    tests = [
        ("æ•°æ®åº“åˆå§‹åŒ–", test_database_init),
        ("URLæ„å»º", test_url_building),
        ("æ—¶é—´è§£æ", test_time_parsing),
        ("CSVå¯¼å‡º", test_csv_export),
        # ("åŸºæœ¬çˆ¬å–", test_basic_scraping),  # æ³¨é‡Šæ‰å®é™…çˆ¬å–æµ‹è¯•ï¼Œé¿å…é¢‘ç¹è®¿é—®ç½‘ç«™
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        try:
            if test_func():
                passed += 1
        except Exception as e:
            print(f"âŒ {test_name}æµ‹è¯•å‡ºç°å¼‚å¸¸: {e}")
    
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³åŠŸèƒ½")
    
    return passed == total


if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1) 